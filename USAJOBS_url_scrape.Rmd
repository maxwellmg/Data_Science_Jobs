---
title: "USAJOBS Scrape"
author: "Maxwell Miller-Golub"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
options(max.print=1000000)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
```

```{r urls}
# Each page for the urls below contain 25 hits for public sector jobs.

#keywords: Data Analyst
url1 = "https://www.usajobs.gov/Search/Results?jt=Data%20Analyst"
#keywords: Data Engineer
url2 = "https://www.usajobs.gov/Search/Results?jt=Data%20Engineer"
#keywords: Data Scientist
url3 = "https://www.usajobs.gov/Search/Results?jt=Data%20Scientist"

```


```{r selenium}
library(tidyverse)
library(RSelenium)
library(rvest)
library(netstat)
library(data.table)
library(wdman)
rs_driver_object <- rsDriver(browser = c("chrome", "firefox", "phantomjs", "internet explorer"),
                             chromever = NULL,
                             geckover = "latest",
                             iedrver = NULL,
                             phantomver = "2.1.1",
                             verbose = FALSE,
                             check = TRUE,
                             port = free_port())

remDr <- rs_driver_object$client

remDr$open()
remDr$navigate(url2)

data_scientist_url_list <- list()
data_analyst_url_list <- list()
data_engineer_url_list <- list()

cond <- TRUE

while (cond == TRUE) {
Sys.sleep(0.5)
  
job_list_rough <- remDr$getPageSource()[[1]] %>%
  read_html() %>% html_nodes('a') %>% html_attr('href') 

page_urls <- list()

for (i in job_list_rough){
  if (grepl("/job/", i) == TRUE){
    page_urls <- append(page_urls, i)
  }
}
data_engineer_url_list <- rbindlist(list(data_engineer_url_list, page_urls))

tryCatch(
  {
    next_button <- remDr$findElement(using = "xpath", '//a[@class="usajobs-search-pagination__next-page"]')
    next_button$clickElement()
  },
  error=function(e) {
    print("Script Complete!")
    cond <<- FALSE
  }
)
if (cond == FALSE){
  break
}
}

data_engineer_url_list
```

```{r output_csv}
# The following couple lines of code can't just be repeated. The extra page urls from running the code chunk above will be added back to the data before exporting it to txt files below.

#extra_data_scientist_urls <- page_urls
#extra_data_analyst_urls <- page_urls
#extra_data_engineer_urls <- page_urls


# Time to export. After establishing the "leading url" to add to the beginning of each href, the code will append the last couple of urls that didn't fit (because rbindlist was looking for exactly 25 inputs and the last page didnt have 25). After appending the final urls, capture.output creates an output file of all urls. The result is 3 text files with (likely some overlapping) urls that each lead to a single job posting. The next step will take each url and scrape it for pertinent information

leading_url <- "https://www.usajobs.gov"



appendable_analyst_url_list <- c()
for (section in data_analyst_url_list){
  for (row in section) {
    for (i in row) {
      i <- paste(leading_url, i, sep = "")
      appendable_analyst_url_list <- append(appendable_analyst_url_list, i)
    }
  }
}


for (row in extra_data_analyst_urls){
  for (i in row) {
    i <- paste(leading_url, i, sep = "")
    appendable_analyst_url_list <- append(appendable_analyst_url_list, i)
  }
}

appendable_analyst_url_list


capture.output(appendable_analyst_url_list, file = "data_analyst_urls.txt")

######################################################################

# Do the same for data scientist list

appendable_scientist_url_list <- c()
for (section in data_scientist_url_list){
  for (row in section) {
    for (i in row) {
      i <- paste(leading_url, i, sep = "")
      appendable_scientist_url_list <- append(appendable_scientist_url_list, i)
    }
  }
}


for (row in extra_data_scientist_urls){
  for (i in row) {
    i <- paste(leading_url, i, sep = "")
    appendable_scientist_url_list <- append(appendable_scientist_url_list, i)
  }
}

capture.output(appendable_scientist_url_list, file = "data_scientist_urls.txt")


######################################################################
# Do the same for data engineer list

appendable_engineer_url_list <- c()
for (section in data_engineer_url_list){
  for (row in section) {
    for (i in row) {
      i <- paste(leading_url, i, sep = "")
      appendable_engineer_url_list <- append(appendable_engineer_url_list, i)
    }
  }
}


for (row in extra_data_engineer_urls){
  for (i in row) {
    i <- paste(leading_url, i, sep = "")
    appendable_engineer_url_list <- append(appendable_engineer_url_list, i)
  }
}


capture.output(appendable_engineer_url_list, file = "data_engineer_urls.txt")

```









